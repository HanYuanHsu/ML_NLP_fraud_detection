{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d43e30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yunjaecho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools \n",
    "import os\n",
    "import seaborn as sns\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk.corpus # sample text for performing tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fa5f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sean's parsing function\n",
    "\n",
    "def parsing(dat,train_dat):\n",
    "    \n",
    "    ### SALARY PROCESSING \n",
    "    # see if character is in text\n",
    "    def alpha_in_text(text):\n",
    "        return(any(c.isalpha() for c in text))\n",
    "\n",
    "    # see how many dashes are in text\n",
    "    def number_of_dashes(text):\n",
    "        return(sum([1 for i in text if '-' in i]))\n",
    "\n",
    "    # extract smallest salary range value\n",
    "    def salary_extract_first(text):\n",
    "\n",
    "        if pd.isna(text) is True:\n",
    "            return(-1)\n",
    "\n",
    "        elif alpha_in_text(text) is True:\n",
    "            return(-2)\n",
    "\n",
    "        elif '-' in text:\n",
    "            if number_of_dashes(text) == 1:\n",
    "                if re.split('-',text)[0].isdigit() is True:\n",
    "                    return(float(re.split('-',text)[0]))\n",
    "                else:\n",
    "                    return(-1)\n",
    "\n",
    "            else:\n",
    "                return(-1)\n",
    "        else:\n",
    "            return(-1)\n",
    "\n",
    "    # largest salary range value\n",
    "    def salary_extract_second(text):\n",
    "\n",
    "        if pd.isna(text) is True:\n",
    "            return(-1)\n",
    "\n",
    "        elif alpha_in_text(text) is True:\n",
    "            return(-2)\n",
    "\n",
    "        elif '-' in text:\n",
    "            if number_of_dashes(text) == 1:\n",
    "                if re.split('-',text)[1].isdigit() is True:\n",
    "                    return(float(re.split('-',text)[1]))\n",
    "                else:\n",
    "                    return(-1)\n",
    "\n",
    "            else:\n",
    "                return(-1)\n",
    "        else:\n",
    "            return(-1)\n",
    "\n",
    "    # convert numeric salary to category\n",
    "    def salary_category_first(number):\n",
    "        percentile = [60.0, 14000.0, 20000.0, 30000.0, 35000.0, 44374.4, 55000.0, 70000.0, 90000.0]\n",
    "        if number == -1:\n",
    "            return(str(1))\n",
    "\n",
    "        if number == -2:\n",
    "            return(str(2))\n",
    "\n",
    "        for i in range(len(percentile)):\n",
    "            if i not in {0,8}:\n",
    "                if (number > percentile[i-1]) & (number <= percentile[i]):\n",
    "                    return(str(i+3))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if i == 0:\n",
    "                if number < percentile[0]:\n",
    "                    return(str(i+3))\n",
    "            if i == 8:\n",
    "                if number >= percentile[8]:\n",
    "                    return(str(i+3))\n",
    "\n",
    "\n",
    "\n",
    "    def salary_category_second(number):\n",
    "        percentile = [120, 20000.0, 30000.0, 40000.0, 50000.0, 65000.0, 80000.0, 100000.0, 130000.0]\n",
    "        if number == -1:\n",
    "            return(str(1))\n",
    "\n",
    "        if number == -2:\n",
    "            return(str(2))\n",
    "\n",
    "        for i in range(len(percentile)):\n",
    "            if i not in {0,8}:\n",
    "                if (number > percentile[i-1]) & (number <= percentile[i]):\n",
    "                    return(str(i+3))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if i == 0:\n",
    "                if number < percentile[0]:\n",
    "                    return(str(i+3))\n",
    "            if i == 8:\n",
    "                if number >= percentile[8]:\n",
    "                    return(str(i+3))\n",
    "    \n",
    "    \n",
    "    ### ONE HOT ENCODING (training)\n",
    "    employment_type_onehot = OneHotEncoder(handle_unknown='ignore').fit(train_dat[['employment_type']].fillna('NaN'))\n",
    "    required_experience_onehot = OneHotEncoder(handle_unknown='ignore').fit(train_dat[['required_experience']].fillna('NaN'))\n",
    "    required_education_onehot = OneHotEncoder(handle_unknown='ignore').fit(train_dat[['required_education']].fillna('NaN'))\n",
    "    industry_onehot = OneHotEncoder(handle_unknown='ignore').fit(train_dat[['industry']].fillna('NaN'))\n",
    "    function_onehot = OneHotEncoder(handle_unknown='ignore').fit(train_dat[['function.']].fillna('NaN'))\n",
    "    category_1 = train_dat.salary_range.apply(salary_extract_first).apply(salary_category_first)\n",
    "    category_2 = train_dat.salary_range.apply(salary_extract_second).apply(salary_category_second)\n",
    "    salary_1_onehot = OneHotEncoder(handle_unknown='ignore').fit(pd.DataFrame(category_1))\n",
    "    salary_2_onehot = OneHotEncoder(handle_unknown='ignore').fit(pd.DataFrame(category_2))\n",
    "    \n",
    "    ### OTHER PARSING\n",
    "    nacols = dat.isna()[['title', 'location', 'department', 'salary_range','description', 'requirements', 'benefits',\n",
    "                      'telecommuting', 'has_company_logo', 'has_questions', 'employment_type',\n",
    "                      'required_experience', 'required_education', 'industry', 'function.']].astype('int')\n",
    "    \n",
    "    numeric_cols = dat[['telecommuting', 'has_company_logo', 'has_questions']]\n",
    "    # func to count words in document\n",
    "    document_word_count = lambda document: len(document.split(' '))\n",
    "    \n",
    "    # count words in column\n",
    "    columns = [\"company_profile\",\"description\",\"requirements\",\"benefits\"]\n",
    "    df = copy.deepcopy(dat[columns])\n",
    "    for column in columns:\n",
    "            df[(str(column) + \"_length\")] = dat[column].apply(lambda x: len(x) if x == x else 0)\n",
    "    \n",
    "    \n",
    "    # salary column one hot\n",
    "    category_1 = dat.salary_range.apply(salary_extract_first).apply(salary_category_first)\n",
    "    category_2 = dat.salary_range.apply(salary_extract_second).apply(salary_category_second)\n",
    "    \n",
    "    salary_1_transform = pd.DataFrame.sparse.from_spmatrix(salary_1_onehot.transform(pd.DataFrame(category_1)))\n",
    "    salary_2_transform = pd.DataFrame.sparse.from_spmatrix(salary_2_onehot.transform(pd.DataFrame(category_2)))\n",
    "    \n",
    "    # transform to one hot\n",
    "    employment_type_transformed =  pd.DataFrame.sparse.from_spmatrix(employment_type_onehot.transform(dat[['employment_type']].fillna('NaN')))\n",
    "    required_experience_transformed =  pd.DataFrame.sparse.from_spmatrix(required_experience_onehot.transform(dat[['required_experience']].fillna('NaN')))\n",
    "    required_education_transformed =  pd.DataFrame.sparse.from_spmatrix(required_education_onehot.transform(dat[['required_education']].fillna('NaN')))\n",
    "    industry_transformed =  pd.DataFrame.sparse.from_spmatrix(industry_onehot.transform(dat[['industry']].fillna('NaN')))\n",
    "    function_transformed =  pd.DataFrame.sparse.from_spmatrix(function_onehot.transform(dat[['function.']].fillna('NaN')))\n",
    "    \n",
    "    \n",
    "    return(pd.concat([nacols,salary_1_transform, salary_2_transform,df.iloc[:,4:],\n",
    "                      employment_type_transformed, required_experience_transformed, required_education_transformed, industry_transformed,function_transformed,numeric_cols],axis = 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6a29f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = pd.read_csv(\"job_training_data.csv\")\n",
    "dat_train2 = pd.read_csv(\"practice_job_verification_data.csv\")\n",
    "X1 = parsing(dat_train,dat_train)\n",
    "X2 = parsing(dat_train2,dat_train)\n",
    "y1 = dat_train[\"fraudulent\"]\n",
    "y2 = dat_train2[\"fraudulent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "541c46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_counter(df,columns = [\"company_profile\",\"description\",\"requirements\",\"benefits\"]):\n",
    "    length = []\n",
    "    for column in columns:\n",
    "        df[(str(column) + \"_length\")] = df[column].apply(lambda x: len(x) if x == x else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e77abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv(\"job_training_data.csv\")\n",
    "jobs = length_counter(jobs)\n",
    "nf_jobs = jobs[jobs[\"fraudulent\"] == 0]\n",
    "f_jobs = jobs[jobs[\"fraudulent\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5331a",
   "metadata": {},
   "source": [
    "# frequency selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "d561ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    \n",
    "    \".\",\n",
    "    'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'after',\n",
    " 'again',\n",
    " 'against',\n",
    " 'ain',\n",
    " 'all',\n",
    " 'am',\n",
    " 'an',\n",
    " 'and',\n",
    " 'any',\n",
    " 'are',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'because',\n",
    " 'been',\n",
    " 'before',\n",
    " 'being',\n",
    " 'below',\n",
    " 'between',\n",
    " 'both',\n",
    " 'but',\n",
    " 'by',\n",
    " 'can',\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'd',\n",
    " 'did',\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'do',\n",
    " 'does',\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'doing',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'down',\n",
    " 'during',\n",
    " 'each',\n",
    " 'few',\n",
    " 'for',\n",
    " 'from',\n",
    " 'further',\n",
    " 'had',\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'has',\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'have',\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'having',\n",
    " 'he',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'into',\n",
    " 'is',\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'll',\n",
    " 'm',\n",
    " 'ma',\n",
    " 'me',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'more',\n",
    " 'most',\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'my',\n",
    " 'myself',\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'now',\n",
    " 'o',\n",
    " 'of',\n",
    " 'off',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'or',\n",
    " 'other',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 're',\n",
    " 's',\n",
    " 'same',\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'so',\n",
    " 'some',\n",
    " 'such',\n",
    " 't',\n",
    " 'than',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'the',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'there',\n",
    " 'these',\n",
    " 'they',\n",
    " 'this',\n",
    " 'those',\n",
    " 'through',\n",
    " 'to',\n",
    " 'too',\n",
    " 'under',\n",
    " 'until',\n",
    " 'up',\n",
    " 've',\n",
    " 'very',\n",
    " 'was',\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'we',\n",
    " 'were',\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'what',\n",
    " 'when',\n",
    " 'where',\n",
    " 'which',\n",
    " 'while',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",\n",
    " 'y',\n",
    " 'you',\n",
    " \"you'd\",\n",
    " \"you'll\",\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\"us\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8cfeb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strips(x):\n",
    "    return (x.strip().strip(\"'\").strip('\"').strip(\",\").strip(\"(\").strip(\")\").strip(\".\").strip(\";\").strip(\":\"))\n",
    "\n",
    "def merge_and_tokenize(df,column = \"company_profile\",k = 0):\n",
    "    merged_text = []\n",
    "    text_dict = {}\n",
    "    parsed_row = []\n",
    "    for sentence in df[column]:\n",
    "        if sentence == sentence:\n",
    "            words = sentence.lower().split()\n",
    "            words = [strips(w) for w in words if strips(w) not in stop_words]\n",
    "            for word in words: # remove stop words\n",
    "                if \"url\" in str(word):\n",
    "                    words.remove(word)\n",
    "\n",
    "            parsed_row.append(words)\n",
    "            merged_text.extend(words)\n",
    "        else:\n",
    "            parsed_row.append(np.nan)\n",
    "            \n",
    "    fdist = FreqDist(merged_text)\n",
    "    \n",
    "    keys = list(fdist.keys())\n",
    "    \n",
    "    for key in keys:\n",
    "        if fdist[key] <= k:\n",
    "            del fdist[key]\n",
    "    total_counts = sum(fdist.values())\n",
    "    for key in fdist:\n",
    "        fdist[key] = [fdist[key],fdist[key]/total_counts]\n",
    "    return fdist,parsed_row\n",
    "\n",
    "def fraud_frequency(fraud_dict,parsed_rows):\n",
    "    freq_list = []\n",
    "    \n",
    "    for text in tqdm(parsed_rows):\n",
    "        parsed_dict = {key:0 for key in fraud_dict.keys()}\n",
    "        if text == text:\n",
    "            for word in text:\n",
    "                if word in parsed_dict:\n",
    "                    parsed_dict[word] += 1/len(text)\n",
    "        else:\n",
    "            freq_list.append(parsed_dict)\n",
    "            continue\n",
    "        \n",
    "        freq_list.append(parsed_dict)\n",
    "        \n",
    "    return pd.DataFrame(freq_list)\n",
    "\n",
    "def fraud_freq_dictionary(fraud_dict,nfraud_dict, th_percentage = 0.2,tops = 5):\n",
    "    differenced_farud = {}\n",
    "    for key in fraud_dict.keys():\n",
    "        if key in nfraud_dict.keys():\n",
    "            if abs(fraud_dict[key][1] - nfraud_dict[key][1]) >= fraud_dict[key][1]*th_percentage:\n",
    "                differenced_farud[key] = fraud_dict[key]\n",
    "        if key not in nfraud_dict.keys():\n",
    "            differenced_farud[key] = fraud_dict[key]\n",
    "                   \n",
    "    sorted_differenced_fraud = sorted(differenced_farud.items(), key=operator.itemgetter(1),reverse = True)\n",
    "    #out = dict(itertools.islice(sorted_differenced_farud.items(), tops)) \n",
    "    out = dict(sorted_differenced_fraud[:tops])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ff9d461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_freq_generator(train,test,column,th_percentage,th_count,tops,train_parsing = True):\n",
    "    if train_parsing:\n",
    "        _,parsed_rows = merge_and_tokenize(train,column,th_count*5)\n",
    "        fraud_dict,_ = merge_and_tokenize(train[train[\"fraudulent\"] == 1],column ,th_count)\n",
    "        nfraud_dict,_ = merge_and_tokenize(train[train[\"fraudulent\"] == 0],column ,th_count*5)\n",
    "        filtered_fraud_dict = fraud_freq_dictionary(fraud_dict,nfraud_dict,th_percentage,tops)\n",
    "        fraud_freq = fraud_frequency(filtered_fraud_dict,parsed_rows)\n",
    "    else:\n",
    "        _,parsed_rows = merge_and_tokenize(test,column,th_count*5)\n",
    "        fraud_dict,_ = merge_and_tokenize(train[train[\"fraudulent\"] == 1],column ,th_count)\n",
    "        nfraud_dict,_ = merge_and_tokenize(train[train[\"fraudulent\"] == 0],column ,th_count*5)\n",
    "        filtered_fraud_dict = fraud_freq_dictionary(fraud_dict,nfraud_dict,th_percentage,tops)\n",
    "        fraud_freq = fraud_frequency(filtered_fraud_dict,parsed_rows)\n",
    "    \n",
    "    return fraud_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "157bb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"job_training_data.csv\")\n",
    "test_data = pd.read_csv(\"job_verification_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "c839306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = length_counter(jobs)\n",
    "test_data = length_counter(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "7ffefce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len_df = train_data.loc[:,[\"company_profile_length\", \"description_length\",\"requirements_length\",\"benefits_length\"]]\n",
    "test_len_df = test_data.loc[:,[\"company_profile_length\", \"description_length\",\"requirements_length\",\"benefits_length\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a6faeca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 244627.81it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 146927.24it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 246872.72it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 414338.11it/s]\n"
     ]
    }
   ],
   "source": [
    "company_freq = fraud_freq_generator(train_data,test_data,\"company_profile\",th_percentage = 0.25, th_count = 15, tops = 10)\n",
    "description_freq = fraud_freq_generator(train_data,test_data,\"description\",th_percentage = 0.25, th_count = 15, tops = 10)\n",
    "requirements_freq = fraud_freq_generator(train_data,test_data,\"requirements\",th_percentage = 0.25,th_count = 15, tops = 10)\n",
    "benefits_freq = fraud_freq_generator(train_data,test_data,\"benefits\",th_percentage = 0.25, th_count = 15, tops = 10)\n",
    "\n",
    "big_matrix = pd.concat([company_freq,description_freq,requirements_freq,benefits_freq,train_len_df],axis=1)\n",
    "big_matrix[\"fraudulent\"] = train_data[\"fraudulent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8b385d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1449   94]\n",
      " [  31   35]]\n",
      "0.9223119950279677\n",
      "0.5303030303030303\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = big_matrix\n",
    "\n",
    "\n",
    "X = feature_matrix.drop(\"fraudulent\", axis = 1)\n",
    "y = feature_matrix.fraudulent\n",
    "y = y.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state =999999999)\n",
    "rfm = RandomForestClassifier()\n",
    "rfm = RandomForestClassifier(n_estimators = 200, max_features=None ,oob_score=True,n_jobs=6)\n",
    "rfm.fit(X_train,y_train)\n",
    "#y_pred_rfm=rfm.predict(X_test)\n",
    "y_pred_rfm = (rfm.predict_proba(X_test)[:, 1] > 0.15).astype(int)\n",
    "print(confusion_matrix(y_test,y_pred_rfm))\n",
    "print(accuracy_score(y_test,y_pred_rfm))\n",
    "print(recall_score(y_test,y_pred_rfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "dcef14f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 254485.01it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 132837.13it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 244192.21it/s]\n",
      "100%|███████████████████████████████████| 5362/5362 [00:00<00:00, 399407.86it/s]\n"
     ]
    }
   ],
   "source": [
    "company_freq2 = fraud_freq_generator(train_data,test_data,\"company_profile\",th_percentage = 0.25, th_count = 15, tops = 10,train_parsing =False)\n",
    "description_freq2 = fraud_freq_generator(train_data,test_data,\"description\",th_percentage = 0.25, th_count = 15, tops = 10,train_parsing =False)\n",
    "requirements_freq2 = fraud_freq_generator(train_data,test_data,\"requirements\",th_percentage = 0.25,th_count = 15, tops = 10,train_parsing =False)\n",
    "benefits_freq2 = fraud_freq_generator(train_data,test_data,\"benefits\",th_percentage = 0.25, th_count = 15, tops = 10,train_parsing = False)\n",
    "\n",
    "big_matrix2 = pd.concat([company_freq2,description_freq2,requirements_freq2,benefits_freq2,test_len_df],axis=1)\n",
    "big_matrix2[\"fraudulent\"] = test_data[\"fraudulent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e436bca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>candidates</th>\n",
       "      <th>recruiting</th>\n",
       "      <th>bonus</th>\n",
       "      <th>solutions</th>\n",
       "      <th>experience</th>\n",
       "      <th>products</th>\n",
       "      <th>financing</th>\n",
       "      <th>signing</th>\n",
       "      <th>referral</th>\n",
       "      <th>...</th>\n",
       "      <th>training</th>\n",
       "      <th>environment</th>\n",
       "      <th>time</th>\n",
       "      <th>working</th>\n",
       "      <th>full</th>\n",
       "      <th>+</th>\n",
       "      <th>please</th>\n",
       "      <th>skills</th>\n",
       "      <th>compensation</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     business  candidates  recruiting  bonus  solutions  experience  products  \\\n",
       "0    0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "1    0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "2    0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "3    0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "4    0.041667         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "..        ...         ...         ...    ...        ...         ...       ...   \n",
       "995  0.031250         0.0         0.0    0.0   0.046875    0.000000       0.0   \n",
       "996  0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "997  0.000000         0.0         0.0    0.0   0.000000    0.030303       0.0   \n",
       "998  0.031250         0.0         0.0    0.0   0.046875    0.000000       0.0   \n",
       "999  0.000000         0.0         0.0    0.0   0.000000    0.000000       0.0   \n",
       "\n",
       "     financing  signing  referral  ...  training  environment  time   working  \\\n",
       "0     0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "1     0.000000      0.0       0.0  ...   0.04878     0.000000   0.0  0.000000   \n",
       "2     0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "3     0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "4     0.000000      0.0       0.0  ...   0.00000     0.022727   0.0  0.000000   \n",
       "..         ...      ...       ...  ...       ...          ...   ...       ...   \n",
       "995   0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "996   0.019802      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "997   0.000000      0.0       0.0  ...   0.00000     0.018868   0.0  0.018868   \n",
       "998   0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "999   0.000000      0.0       0.0  ...   0.00000     0.000000   0.0  0.000000   \n",
       "\n",
       "     full    +  please  skills  compensation  fraudulent  \n",
       "0     0.0  0.0     0.0     0.0           0.0           0  \n",
       "1     0.0  0.0     0.0     0.0           0.0           0  \n",
       "2     0.0  0.0     0.0     0.0           0.0           0  \n",
       "3     0.0  0.0     0.0     0.0           0.0           0  \n",
       "4     0.0  0.0     0.0     0.0           0.0           0  \n",
       "..    ...  ...     ...     ...           ...         ...  \n",
       "995   0.0  0.0     0.0     0.0           0.0           0  \n",
       "996   0.0  0.0     0.0     0.0           0.0           0  \n",
       "997   0.0  0.0     0.0     0.0           0.0           0  \n",
       "998   0.0  0.0     0.0     0.0           0.0           0  \n",
       "999   0.0  0.0     0.0     0.0           0.0           1  \n",
       "\n",
       "[1000 rows x 41 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "c8262896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5016   87]\n",
      " [   0  259]]\n",
      "0.9837747109287579\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_matrix = big_matrix\n",
    "test_matrix = big_matrix2\n",
    "\n",
    "X_train = train_matrix.drop(\"fraudulent\", axis = 1)\n",
    "y_train = train_matrix.fraudulent.astype(int)\n",
    "\n",
    "X_test = test_matrix.drop(\"fraudulent\", axis = 1)\n",
    "y_test = test_matrix.fraudulent.astype(int)\n",
    "\n",
    "rfm = RandomForestClassifier()\n",
    "rfm = RandomForestClassifier(n_estimators = 200, max_features=None ,oob_score=True,n_jobs=6)\n",
    "rfm.fit(X_train,y_train)\n",
    "#y_pred_rfm=rfm.predict(X_test)\n",
    "y_pred_rfm = (rfm.predict_proba(X_test)[:, 1] > 0.13).astype(int)\n",
    "print(confusion_matrix(y_test,y_pred_rfm))\n",
    "print(accuracy_score(y_test,y_pred_rfm))\n",
    "print(recall_score(y_test,y_pred_rfm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
