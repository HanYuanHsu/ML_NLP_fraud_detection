{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e99a9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library needed\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c069f4",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2727bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"./job_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c12413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\".\",'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'after',\n",
    " 'again',\n",
    " 'against',\n",
    " 'ain',\n",
    " 'all',\n",
    " 'am',\n",
    " 'an',\n",
    " 'and',\n",
    " 'any',\n",
    " 'are',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'because',\n",
    " 'been',\n",
    " 'before',\n",
    " 'being',\n",
    " 'below',\n",
    " 'between',\n",
    " 'both',\n",
    " 'but',\n",
    " 'by',\n",
    " 'can',\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'd',\n",
    " 'did',\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'do',\n",
    " 'does',\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'doing',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'down',\n",
    " 'during',\n",
    " 'each',\n",
    " 'few',\n",
    " 'for',\n",
    " 'from',\n",
    " 'further',\n",
    " 'had',\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'has',\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'have',\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'having',\n",
    " 'he',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'into',\n",
    " 'is',\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'll',\n",
    " 'm',\n",
    " 'ma',\n",
    " 'me',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'more',\n",
    " 'most',\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'my',\n",
    " 'myself',\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'now',\n",
    " 'o',\n",
    " 'of',\n",
    " 'off',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'or',\n",
    " 'other',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 're',\n",
    " 's',\n",
    " 'same',\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'so',\n",
    " 'some',\n",
    " 'such',\n",
    " 't',\n",
    " 'than',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'the',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'there',\n",
    " 'these',\n",
    " 'they',\n",
    " 'this',\n",
    " 'those',\n",
    " 'through',\n",
    " 'to',\n",
    " 'too',\n",
    " 'under',\n",
    " 'until',\n",
    " 'up',\n",
    " 've',\n",
    " 'very',\n",
    " 'was',\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'we',\n",
    " 'were',\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'what',\n",
    " 'when',\n",
    " 'where',\n",
    " 'which',\n",
    " 'while',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",\n",
    " 'y',\n",
    " 'you',\n",
    " \"you'd\",\n",
    " \"you'll\",\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\"us\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1586ed",
   "metadata": {},
   "source": [
    "# Main Parsing function\n",
    "\n",
    "1. get number of words\n",
    "2. one-hot encode descriptive features and categorical features\n",
    "3. include number of na's for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e0ef7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(dat, stop_words):\n",
    "    \n",
    "    # The four descriptive columns\n",
    "    \n",
    "    # create machine for fitting (not required for testing data)\n",
    "    vectorizer_profile = CountVectorizer(stop_words=stop_words)\n",
    "    vectorizer_description = CountVectorizer(stop_words=stop_words)\n",
    "    vectorizer_requirements = CountVectorizer(stop_words=stop_words)\n",
    "    vectorizer_benefits = CountVectorizer(stop_words=stop_words)\n",
    "    \n",
    "    # Fit word processor (not required for testing data)\n",
    "    vectorizer_profile.fit(dat['company_profile'].fillna('NaN'))\n",
    "    vectorizer_description.fit(dat['description'].fillna('NaN'))\n",
    "    vectorizer_requirements.fit(dat['requirements'].fillna('NaN'))\n",
    "    vectorizer_benefits.fit(dat['benefits'].fillna('NaN'))\n",
    "    \n",
    "    # transforming to one hot encoding\n",
    "    profile_onehot = pd.DataFrame.sparse.from_spmatrix(vectorizer_profile.transform(dat['company_profile'].fillna('NaN')))\n",
    "    description_onehot = pd.DataFrame.sparse.from_spmatrix(vectorizer_description.transform(dat['description'].fillna('NaN')))\n",
    "    requirements_onehot = pd.DataFrame.sparse.from_spmatrix(vectorizer_requirements.transform(dat['requirements'].fillna('NaN')))\n",
    "    benifits_onehot = pd.DataFrame.sparse.from_spmatrix(vectorizer_benefits.transform(dat['benefits'].fillna('NaN')))\n",
    "    print('fitting done')\n",
    "    \n",
    "    # func to count words in document\n",
    "    document_word_count = lambda document: len(document.split(' '))\n",
    "    \n",
    "    # get text length for each descriptive column\n",
    "    profile_count = pd.Series([document_word_count(dat['company_profile'].fillna('NaN')[i]) for i in range(dat.shape[0])])\n",
    "    print('done with profile')\n",
    "    description_count = pd.Series([document_word_count(dat['description'].fillna('NaN')[i]) for i in range(dat.shape[0])])\n",
    "    print('done with description')\n",
    "    requirements_count = pd.Series([document_word_count(dat['requirements'].fillna('NaN')[i]) for i in range(dat.shape[0])])\n",
    "    print('done with requirements')\n",
    "    benifits_count = pd.Series([document_word_count(dat['benefits'].fillna('NaN')[i]) for i in range(dat.shape[0])])\n",
    "    print('text length done')\n",
    "    \n",
    "    '''\n",
    "    # get frequency\n",
    "    profile_freq = (profile_onehot.transpose()/profile_count).transpose()\n",
    "    print('done with profile')\n",
    "    description_freq = (description_onehot.transpose()/description_count).transpose()\n",
    "    print('done with description')\n",
    "    requirements_freq = (requirements_onehot.transpose()/requirements_count).transpose()\n",
    "    print('done with requirements')\n",
    "    benifits_freq = (benifits_onehot.transpose()/benifits_count).transpose()\n",
    "    print('frequency done')\n",
    "    '''\n",
    "    \n",
    "    # concat results\n",
    "    descriptive_summary = pd.concat([profile_onehot,description_onehot,requirements_onehot,benifits_onehot,profile_count,description_count,requirements_count,benifits_count],\n",
    "             axis=1,ignore_index=True)\n",
    "    \n",
    "    # other columns (one-hot)\n",
    "    text = dat.fillna('NAN')[['location','department','salary_range','employment_type','required_experience','required_education','industry','function.']].agg(' '.join, axis=1)\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    # not required for testing\n",
    "    vectorizer.fit(text)\n",
    "    # required for testing\n",
    "    text_onehot = pd.DataFrame.sparse.from_spmatrix(vectorizer.transform(text)) \n",
    "    \n",
    "    # concat all results\n",
    "    data = pd.concat([descriptive_summary,text_onehot,dat[['telecommuting', 'has_company_logo', 'has_questions']],dat.isna().astype('int')],axis = 1,ignore_index=True)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e368fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting done\n",
      "done with profile\n",
      "done with description\n",
      "done with requirements\n",
      "text length done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:515: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_normalized = preprocessing.scale(parsing(dat.iloc[:,:-1],stop_words=stop_words),with_mean=True,with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7a98565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"data_normalized.csv\",data_normalized ,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779197fb",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4b3ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dde9cfdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "028b2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_[0:np.where(pca.explained_variance_ratio_.cumsum() > 0.9)[0][0]+1,:]\n",
    "pca_features = np.matmul(data_normalized,np.transpose(components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9968bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(pca_features).to_csv('pca_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d369b1",
   "metadata": {},
   "source": [
    "# Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bfc7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "157b6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "637382dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_features':['sqrt', 'log2'],\n",
    "             'n_estimators':[200],\n",
    "             'ccp_alpha':[0,0.3,0.5],\n",
    "             'n_jobs':[6]}\n",
    "clf = GridSearchCV(rf, param_grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb1e0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886.3657009601593\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(pca_features,dat.fraudulent)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e265ce78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0, 'max_features': 'sqrt', 'n_estimators': 200, 'n_jobs': 6}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4d610d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(pca_features) == dat.fraudulent)/len(dat.fraudulent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98c4b411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5103,    0],\n",
       "       [   0,  259]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(clf.predict(pca_features),dat.fraudulent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75493b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6.176079750061035\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print()\n",
    "rf = RandomForestClassifier(n_jobs =6 , max_features='log2', n_estimators = 200)\n",
    "rf.fit(pca_features,dat.fraudulent)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15cfc8",
   "metadata": {},
   "source": [
    "# Code to Deal with Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00880282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if character is in text\n",
    "def alpha_in_text(text):\n",
    "    return(any(c.isalpha() for c in text))\n",
    "\n",
    "# see how many dashes are in text\n",
    "def number_of_dashes(text):\n",
    "    return(sum([1 for i in text if '-' in i]))\n",
    "\n",
    "# extract smallest salary range value\n",
    "def salary_extract_first(text):\n",
    "    \n",
    "    if pd.isna(text) is True:\n",
    "        return(-1)\n",
    "    \n",
    "    elif alpha_in_text(text) is True:\n",
    "        return(-2)\n",
    "    \n",
    "    elif '-' in text:\n",
    "        if number_of_dashes(text) == 1:\n",
    "            if re.split('-',text)[0].isdigit() is True:\n",
    "                return(float(re.split('-',text)[0]))\n",
    "            else:\n",
    "                return(-1)\n",
    "            \n",
    "        else:\n",
    "            return(-1)\n",
    "    else:\n",
    "        return(-1)\n",
    "    \n",
    "# largest salary range value\n",
    "def salary_extract_second(text):\n",
    "    \n",
    "    if pd.isna(text) is True:\n",
    "        return(-1)\n",
    "    \n",
    "    elif alpha_in_text(text) is True:\n",
    "        return(-2)\n",
    "    \n",
    "    elif '-' in text:\n",
    "        if number_of_dashes(text) == 1:\n",
    "            if re.split('-',text)[1].isdigit() is True:\n",
    "                return(float(re.split('-',text)[1]))\n",
    "            else:\n",
    "                return(-1)\n",
    "            \n",
    "        else:\n",
    "            return(-1)\n",
    "    else:\n",
    "        return(-1)\n",
    "\n",
    "# convert numeric salary to category\n",
    "def salary_category_first(number):\n",
    "    percentile = [60.0, 14000.0, 20000.0, 30000.0, 35000.0, 44374.4, 55000.0, 70000.0, 90000.0]\n",
    "    if number == -1:\n",
    "        return(str(1))\n",
    "    \n",
    "    if number == -2:\n",
    "        return(str(2))\n",
    "    \n",
    "    for i in range(len(percentile)):\n",
    "        if i not in {0,8}:\n",
    "            if (number > percentile[i-1]) & (number <= percentile[i]):\n",
    "                return(i+3)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        if i == 0:\n",
    "            if number < percentile[0]:\n",
    "                return(str(i+3))\n",
    "        if i == 8:\n",
    "            if number >= percentile[8]:\n",
    "                return(str(i+3))\n",
    "            \n",
    "\n",
    "\n",
    "def salary_category_second(number):\n",
    "    percentile = [120, 20000.0, 30000.0, 40000.0, 50000.0, 65000.0, 80000.0, 100000.0, 130000.0]\n",
    "    if number == -1:\n",
    "        return(str(1))\n",
    "    \n",
    "    if number == -2:\n",
    "        return(str(2))\n",
    "    \n",
    "    for i in range(len(percentile)):\n",
    "        if i not in {0,8}:\n",
    "            if (number > percentile[i-1]) & (number <= percentile[i]):\n",
    "                return(i+3)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        if i == 0:\n",
    "            if number < percentile[0]:\n",
    "                return(str(i+3))\n",
    "        if i == 8:\n",
    "            if number >= percentile[8]:\n",
    "                return(str(i+3))\n",
    "            \n",
    "def convert_salary_to_category(series):\n",
    "    # first column\n",
    "    category_1 = series.apply(salary_extract_first).apply(salary_category_first)\n",
    "    dummy_1 = pd.get_dummies(category_1, drop_first=True)\n",
    "    \n",
    "    # second column\n",
    "    category_2 = series.apply(salary_extract_second).apply(salary_category_second)\n",
    "    dummy_2 = pd.get_dummies(category_2, drop_first=True)\n",
    "    \n",
    "    return(pd.concat([dummy_1,dummy_2],axis=1,ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a987b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "convert_salary_to_category(dat.salary_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3a6bc",
   "metadata": {},
   "source": [
    "# Below are Test code (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dat.salary_range.apply(salary_extract_second).apply(salary_category_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(test,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c5362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low = dat.salary_range.apply(salary_extract_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "up = dat.salary_range.apply(salary_extract_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa6a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([np.percentile(low[low>=0],i*10) for i in range(1,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([np.percentile(up[up>=0],i*10) for i in range(1,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c341e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1---,-asdfasdf109-'\n",
    "sum([1 for i in text if '-' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86333e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split('-','35000-40000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.loc[dat.salary_range=='800000000-1200000000',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55818418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[len(pd.unique(dat.iloc[:,i])) for i in range(dat.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main parsing function\n",
    "# dat: the raw pandas dataframe\n",
    "# stop_words: a set of words to omitt when parsing\n",
    "def parsing(dat, stop_words):\n",
    "    \n",
    "    # let character rows be merged into one single text (only for columns of type 'object')\n",
    "    data = dat.fillna('NaN')\n",
    "    data_object_colnames = data.columns[data.dtypes == 'object']\n",
    "    text = data[data_object_colnames].agg(' '.join, axis=1)\n",
    "    \n",
    "    # results for parsing\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    result = pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(text))\n",
    "    \n",
    "    # merge parsed results, number of NA for each field, numeric columns (less fraud and job id)\n",
    "    cleaned_data = pd.concat([result,\n",
    "                              dat.isna().astype('int'),\n",
    "                              dat.loc[:,dat.dtypes == 'int64'].iloc[:,1:4]], axis = 1, ignore_index=True)\n",
    "    return cleaned_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\"a\", \"the\"}\n",
    "result = parsing(dat, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5406e",
   "metadata": {},
   "source": [
    "# Below are code for thought process (can ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ff64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a text\n",
    "# gets rid of punctuation\n",
    "def ridpunctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca1c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method 1\n",
    "# let character rows be merged into one single text\n",
    "data = dat.fillna(' NaN ')\n",
    "data_object_colnames = data.columns[data.dtypes == 'object']\n",
    "text = data[data_object_colnames[0]]\n",
    "for col in range(1,len(data_object_colnames)):\n",
    "    text = text + data[data_object_colnames[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0db1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "data = dat.fillna('NaN')\n",
    "data_object_colnames = data.columns[data.dtypes == 'object']\n",
    "text = data[data_object_colnames].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results for parsing\n",
    "vectorizer = CountVectorizer()\n",
    "result = pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA info \n",
    "cleaned_data = pd.concat([result ,dat.isna().astype('int'),dat.loc[:,dat.dtypes == 'int64'].iloc[:,1:4]],axis = 1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cde3b",
   "metadata": {},
   "source": [
    "# Below are scratch code (can ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = pd.DataFrame({\"a\":['hello\"s , hello hello,hi you are',\"hello hello today's\\xa0Document Hello\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results of parsing\n",
    "vectorizer = CountVectorizer()\n",
    "result = vectorizer.fit_transform(test_series['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21257217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.sparse.from_spmatrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e987471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = CountVectorizer().fit_transform(data.iloc[:,1])\n",
    "test_names\n",
    "pd.DataFrame.sparse.from_spmatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a51c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parsing_series(series):\n",
    "    sparse_parse = CountVectorizer(stop_words=stop_words).fit_transform(series)\n",
    "    pandas.DataFrame.sparse.from_spmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(col):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat na to \n",
    "pd.concat([data ,data.isna().astype('int')],axis = 1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
